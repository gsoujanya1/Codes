from textblob import TextBlob
import tweepy
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import re

from tweepy.auth import OAuthHandler
from wordcloud import WordCloud, STOPWORDS
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from langdetect import detect
from spellchecker import SpellChecker
from collections import Counter


# Authentication
consumerKey = " INPUT "
consumerSecret = " INPUT "
accessToken = " INPUT "
accessTokenSecret = " INPUT "
auth = OAuthHandler(consumerKey, consumerSecret)
auth.set_access_token(accessToken, accessTokenSecret)
api = tweepy.API(auth,wait_on_rate_limit=True)

#Sentiment Analysis
def percentage(part,whole):
 return 100 * float(part)/float(whole)

keyword = input("Please enter keyword or hashtag to search: ")
noOfTweet = int(input ("Please enter how many tweets to analyze: "))

tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)
positive = 0
negative = 0
neutral = 0
polarity = 0
tweet_list = []
neutral_list = []
negative_list = []
positive_list = []


for tweet in tweets: 
 #print(tweet.text)
 tweet_list.append(tweet.text)
 
 
###Cleaning    
tw_list=pd.DataFrame(tweet_list) 
#Dups
tw_list.drop_duplicates(inplace = True) #Duplicates
tw_list["text"] = tw_list[0]
#Regex
remove_rt = lambda x: re.sub('RT @\w+: '," ",x)
rt = lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x)
tw_list["text"] = tw_list.text.map(remove_rt).map(rt)   
tw_list["text"] = tw_list.text.str.lower() 

#Remove 1+ spaces between words
remove_exspc = lambda x: re.sub(' +', ' ', x)
tw_list["text"] = tw_list.text.map(remove_exspc).map(remove_exspc) 

#Spell-check
spell = SpellChecker(language=None, case_sensitive=True)
tw_list["text"]=tw_list["text"].apply(lambda x: spell.correction(x))

#Remove duplicates in cleaned
tw_list=tw_list.drop_duplicates(subset="text", keep='first', inplace=False)

#Remove Non-English tweets
def detect_en(text):
    try:
        return detect(text) == 'en'
    except:
        return False  
tw_list=tw_list[tw_list['text'].apply(detect_en)] #Remove non-english



###Analysis
tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))

for index, row in tw_list['text'].iteritems():
 score = SentimentIntensityAnalyzer().polarity_scores(row)
 neg = score['neg']
 neu = score['neu']
 pos = score['pos']
 comp = score['compound']
 if neg > pos:
     tw_list.loc[index, 'sentiment'] = "negative"
 elif pos > neg:
     tw_list.loc[index, 'sentiment'] = "positive"
 else:
     tw_list.loc[index, 'sentiment'] = "neutral"
 tw_list.loc[index, 'neg'] = neg
 tw_list.loc[index, 'neu'] = neu
 tw_list.loc[index, 'pos'] = pos
 tw_list.loc[index, 'compound'] = comp


#Creating new data frames for all sentiments (positive, negative and neutral)
tw_list_negative = tw_list[tw_list["sentiment"]=="negative"]
tw_list_positive = tw_list[tw_list["sentiment"]=="positive"]
tw_list_neutral = tw_list[tw_list["sentiment"]=="neutral"]

def count_values_in_column(data,feature):
    total=data.loc[:,feature].value_counts(dropna=False)
    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)
    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])

#Count_values for sentiment
count_values_in_column(tw_list,"sentiment")

# create data for Pie Chart
pc = count_values_in_column(tw_list,"sentiment")
names= pc.index
size=pc["Percentage"]
 
# Create a circle for the center of the plot
#my_circle=plt.Circle( (0,0), 0.7, color='white')
plt.pie(size, labels=names, colors=['green','blue','red'])
plt.savefig('pie.png')


def create_wordcloud(textdict):
 stopwords = set(STOPWORDS)
 wc = WordCloud(background_color="white",
 max_words=30,
 stopwords=stopwords,
 repeat=False)
 wc.generate_from_frequencies(textdict)
 wc.to_file("wc.png")
 print("Word Cloud Saved Successfully")

#extract keywords
tokens = (tw_list["text"].apply(lambda x: nltk.word_tokenize(x))).explode()
tagged = nltk.pos_tag(tokens)
taggednns=[i[0] for i in tagged if i[1] in ["NN","NNS","NNP", "NNPS","FW"]]
D=dict(Counter(taggednns).most_common())
new={ k:v for k, v in D.items() if v>1 }
new={ k:v for k, v in new.items() if len(k)>2 }
create_wordcloud(new)
